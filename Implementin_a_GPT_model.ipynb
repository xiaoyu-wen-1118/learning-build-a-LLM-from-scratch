{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn # All neural network components, nn.Linear, nn.Conv2d, BatchNorm, Loss functions, Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 1024, # max number of input tokens the model can handle via the positional embeddings\n",
    "    \"embedding_size\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A placeholder GPT model architecture class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__() # Call the __init__ of the parent class\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_size\"]) \n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_size\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])] # * is used to unpack the list\n",
    "        )\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"embedding_size\"])\n",
    "        self.out_head = nn.Linear(cfg[\"embedding_size\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, input_idx):\n",
    "        batch_size, seq_len = input_idx.shape\n",
    "        tok_embeds = self.tok_emb(input_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, # Indices for the positional embeddings\n",
    "            device=input_idx.device) # Ensure the tensor generated by torch.arange is on the same device as input_idx\n",
    "         ) \n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x) # logits are the raw, unnormalized scores or outputs of the final layer before the activation function is applied\n",
    "        return logits\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. High-level overview of data flow in a GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6109, 3626, 6100, 345]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "batch = []\n",
    "tx1 = \"Every effort moves you\"\n",
    "tx2  = \"Every day holds a\"\n",
    "\n",
    "print(tokenizer.encode(tx1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "batch.append(torch.tensor(tokenizer.encode(tx1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(tx2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)\n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.9289,  0.2748, -0.7557,  ..., -1.6070,  0.2702, -0.5888],\n",
      "         [-0.4476,  0.1726,  0.5354,  ..., -0.3932,  1.5285,  0.8557],\n",
      "         [ 0.5680,  1.6053, -0.2155,  ...,  1.1624,  0.1380,  0.7425],\n",
      "         [ 0.0447,  2.4787, -0.8843,  ...,  1.3219, -0.0864, -0.5856]],\n",
      "\n",
      "        [[-1.5474, -0.0542, -1.0571,  ..., -1.8061, -0.4494, -0.6747],\n",
      "         [-0.8422,  0.8243, -0.1098,  ..., -0.1434,  0.2079,  1.2046],\n",
      "         [ 0.1355,  1.1858, -0.1453,  ...,  0.0869, -0.1590,  0.1552],\n",
      "         [ 0.1666, -0.8138,  0.2307,  ...,  2.5035, -0.3055, -0.3083]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Layer Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1115,  0.1204, -0.3696, -0.2404, -1.1969],\n",
      "        [ 0.2093, -0.9724, -0.7550,  0.3239, -0.1085]])\n",
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "torch.manual_seed(123)\n",
    "batch_sample = torch.randn(2,5)\n",
    "print(batch_sample)\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU()) # A linear layer followed by a ReLU activation function\n",
    "out = layer(batch_sample)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var) # Normalizing the output by subtracting the mean and dividing by the standard deviation\n",
    "normalized_mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "nomarlized_var = out_norm.var(dim=-1, keepdim=True)\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", normalized_mean)\n",
    "print(\"Variance:\\n\", nomarlized_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5 # Epsilon value to avoid division by zero\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim)) # Trainable scale parameter, initialized to 1\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim)) # Trainable shift parameter, initialized to 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) # x shape is (batch_size, seq_len, emb_dim), so we calculate the mean along the last dimension - the embedding dimension\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # unbiased=False means the variance is calculated with N instead of N-1\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return norm_x * self.scale + self.shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0.], requires_grad=True))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Parameter(torch.ones(5)), nn.Parameter(torch.zeros(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5528,  1.0693, -0.0223,  0.2656, -1.8654],\n",
      "        [ 0.9087, -1.3767, -0.9564,  1.1304,  0.2940]], grad_fn=<AddBackward0>)\n",
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_sample) # Call the forward method of the LayerNorm instance\n",
    "print(out_ln)\n",
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1,unbiased=False, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BatchNorm1dCustom(nn.Module):\n",
    "    def __init__(self, num_features, epsilon=1e-5, momentum=0.1):\n",
    "        super(BatchNorm1dCustom, self).__init__()\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.num_features = num_features\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        # Learnable parameters (gamma and beta)\n",
    "        self.gamma = nn.Parameter(torch.ones(num_features))  # scale factor\n",
    "        self.beta = nn.Parameter(torch.zeros(num_features))  # shift factor\n",
    "        \n",
    "        # Running mean and variance for inference phase\n",
    "        self.running_mean = torch.zeros(num_features)\n",
    "        self.running_var = torch.ones(num_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Calculate the mean and variance of the current batch\n",
    "        batch_mean = x.mean(dim=0)\n",
    "        batch_var = x.var(dim=0, unbiased=False)\n",
    "        \n",
    "        # Normalize the input using batch statistics\n",
    "        x_normalized = (x - batch_mean) / torch.sqrt(batch_var + self.epsilon)\n",
    "        \n",
    "        # Scale and shift\n",
    "        out = self.gamma * x_normalized + self.beta\n",
    "        \n",
    "        # Update running statistics for inference\n",
    "        # During training, we use the statistics of the current batch (mean and variance) to normalize the data.\n",
    "        # During inference, we don't have a batch of data to compute the mean and variance, \n",
    "        # so we use the running statistics accumulated during training to normalize the input data. \n",
    "        # This helps in ensuring that the model behaves consistently during inference, \n",
    "        # even when the batch size may be different or a single sample is passed through.\n",
    "\n",
    "        # self.momentum is a hyperparameter that controls how quickly the running statistics adapt to new data. \n",
    "        # It determines how much weight is given to the current batch's statistics versus the accumulated statistics\n",
    "        self.running_mean = self.momentum * batch_mean + (1 - self.momentum) * self.running_mean # Cumulative mean across all batches\n",
    "        self.running_var = self.momentum * batch_var + (1 - self.momentum) * self.running_var # Cumulative variance across all batches\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.bn1 = BatchNorm1dCustom(50)  # Apply BatchNorm after fc1\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))  # Pass through FC and BatchNorm\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Example usage\n",
    "x = torch.randn(32, 10)  # A batch of 32 samples, each with 10 features\n",
    "model = SimpleNet()\n",
    "output = model(x)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. GELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh((torch.sqrt(torch.tensor(2 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], grad_fn=<ReluBackward0>)\n",
      "tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([0.0], requires_grad=True)\n",
    "output = torch.relu(x)\n",
    "print(output)\n",
    "output.backward()\n",
    "\n",
    "print(x.grad) # Output: tensor([0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"embedding_size\"], 4*cfg[\"embedding_size\"]),\n",
    "            GELU(), # Apply the GELU activation function to the output of the linear layer\n",
    "            nn.Linear(4*cfg[\"embedding_size\"], cfg[\"embedding_size\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "x = torch.rand(2, 3, 768)\n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
